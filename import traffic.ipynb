{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier téléchargé : comptages_routiers_2020.zip\n",
      "Deux premiers fichiers extraits : ['trafic_capteurs_2020_W00_20200101_20200108.txt', 'trafic_capteurs_2020_W01_20200108_20200115.txt']\n",
      "Fichier converti en JSON : extracted_data/trafic_capteurs_2020_W00_20200101_20200108.json\n",
      "Fichier converti en JSON : extracted_data/trafic_capteurs_2020_W01_20200108_20200115.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# URL du fichier ZIP\n",
    "url = \"https://opendata.paris.fr/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2020_zip/\"\n",
    "\n",
    "# Nom local du fichier ZIP\n",
    "zip_filename = \"comptages_routiers_2020.zip\"\n",
    "\n",
    "# Dossier où extraire les fichiers\n",
    "output_folder = \"extracted_data\"\n",
    "\n",
    "# Étape 1 : Télécharger le fichier ZIP\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Fichier téléchargé : {zip_filename}\")\n",
    "else:\n",
    "    print(f\"Erreur lors du téléchargement : {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Étape 2 : Extraire le contenu du fichier ZIP\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    all_files = zip_ref.namelist()[:2]  # Limiter à 2 fichiers\n",
    "    for file in all_files:\n",
    "        zip_ref.extract(file, output_folder)\n",
    "    print(f\"Deux premiers fichiers extraits : {all_files}\")\n",
    "\n",
    "# Étape 3 : Convertir les fichiers texte en JSON\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(output_folder, file)\n",
    "    if file.endswith(\".txt\"):  # Assurez-vous que le fichier est un fichier texte\n",
    "        try:\n",
    "            json_data = []\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                # Lire le fichier ligne par ligne et convertir en JSON\n",
    "                for line in f:\n",
    "                    data = line.strip().split(\"\\t\")  # Séparer les données par tabulation\n",
    "                    json_data.append(data)\n",
    "            \n",
    "            # Sauvegarder les données en JSON\n",
    "            json_filename = file_path.replace(\".txt\", \".json\")\n",
    "            with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(json_data, json_file, indent=4)\n",
    "            print(f\"Fichier converti en JSON : {json_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du fichier {file}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de la table référentiel en csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier téléchargé et sauvegardé sous : referentiel.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL du fichier CSV que vous voulez télécharger\n",
    "url = 'https://parisdata.opendatasoft.com/api/explore/v2.1/catalog/datasets/referentiel-comptages-routiers/exports/csv?lang=fr&timezone=Europe%2FBerlin&use_labels=true&delimiter=%3B'\n",
    "\n",
    "# Nom local sous lequel vous voulez sauvegarder le fichier\n",
    "local_filename = 'referentiel.csv'\n",
    "\n",
    "# Télécharger le fichier\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier si la demande a réussi (status_code 200)\n",
    "if response.status_code == 200:\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Fichier téléchargé et sauvegardé sous : {local_filename}\")\n",
    "else:\n",
    "    print(f\"Erreur lors du téléchargement du fichier : {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identifiant arc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6799</td>\n",
       "      <td>48.88610804018005</td>\n",
       "      <td>2.3060170797614274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6839</td>\n",
       "      <td>48.882846124767354</td>\n",
       "      <td>2.309608336489109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6381</td>\n",
       "      <td>48.88185021600981</td>\n",
       "      <td>2.313636877615184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240</td>\n",
       "      <td>48.867653082202935</td>\n",
       "      <td>2.362827833173667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5164</td>\n",
       "      <td>48.83748471020789</td>\n",
       "      <td>2.2572392666841106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6603</td>\n",
       "      <td>48.82680441725183</td>\n",
       "      <td>2.3040355371176604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6593</td>\n",
       "      <td>48.86194217627273</td>\n",
       "      <td>2.31342704120477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6813</td>\n",
       "      <td>48.85824539762091</td>\n",
       "      <td>2.314401975386383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4382</td>\n",
       "      <td>48.87468482154655</td>\n",
       "      <td>2.303280892510164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>269</td>\n",
       "      <td>48.86779977023447</td>\n",
       "      <td>2.314350336430965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Identifiant arc                 lat                  lon\n",
       "0             6799   48.88610804018005   2.3060170797614274\n",
       "1             6839  48.882846124767354    2.309608336489109\n",
       "2             6381   48.88185021600981    2.313636877615184\n",
       "3             1240  48.867653082202935    2.362827833173667\n",
       "4             5164   48.83748471020789   2.2572392666841106\n",
       "5             6603   48.82680441725183   2.3040355371176604\n",
       "6             6593   48.86194217627273     2.31342704120477\n",
       "7             6813   48.85824539762091    2.314401975386383\n",
       "8             4382   48.87468482154655    2.303280892510164\n",
       "9              269   48.86779977023447    2.314350336430965"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Charger le fichier du référentiel avec les informations géographiques\n",
    "geo_df = pd.read_csv(\"referentiel.csv\", delimiter=\";\")\n",
    "geo_df[['lat', 'lon']] = geo_df['geo_point_2d'].str.split(',', expand=True)\n",
    "geo_df = geo_df[['Identifiant arc','lat', 'lon']]\n",
    "# Afficher les premières lignes pour vérifier la structure\n",
    "geo_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier trafic_capteurs_2020_W00_20200101_20200108.txt chargé avec succès\n",
      "   iu_ac    k                 t_1h\n",
      "0    799  0.0  2020-01-01 01:00:00\n",
      "1    799  0.0  2020-01-01 02:00:00\n",
      "2    799  0.0  2020-01-01 03:00:00\n",
      "3    799  0.0  2020-01-01 04:00:00\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trafic_df = pd.read_csv(\"extracted_data/trafic_capteurs_2020_W00_20200101_20200108.txt\", delimiter=\";\")\n",
    "    print(\"Fichier trafic_capteurs_2020_W00_20200101_20200108.txt chargé avec succès\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du fichier trafic_capteurs_2020_W00_20200101_20200108.txt : {e}\")\n",
    "\n",
    "trafic_df = trafic_df[['iu_ac','k','t_1h']]\n",
    "trafic_df ['k'] = trafic_df ['k'].fillna(0)\n",
    "print(trafic_df.head(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iu_ac</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6799</td>\n",
       "      <td>48.88610804018005</td>\n",
       "      <td>2.3060170797614274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6839</td>\n",
       "      <td>48.882846124767354</td>\n",
       "      <td>2.309608336489109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6381</td>\n",
       "      <td>48.88185021600981</td>\n",
       "      <td>2.313636877615184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240</td>\n",
       "      <td>48.867653082202935</td>\n",
       "      <td>2.362827833173667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iu_ac                 lat                  lon\n",
       "0   6799   48.88610804018005   2.3060170797614274\n",
       "1   6839  48.882846124767354    2.309608336489109\n",
       "2   6381   48.88185021600981    2.313636877615184\n",
       "3   1240  48.867653082202935    2.362827833173667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df.rename(columns={\"Identifiant arc\": \"iu_ac\"}, inplace=True)\n",
    "geo_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire la jointure sur un identifiant commun (par exemple, 'iu_ac')\n",
    "merged_df = pd.merge(trafic_df, geo_df, on='iu_ac', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iu_ac      int64\n",
      "k        float64\n",
      "t_1h      object\n",
      "lat       object\n",
      "lon       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Afficher les premières lignes du DataFrame fusionné\n",
    "merged_df.head(2)\n",
    "print(merged_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Charger le fichier GeoJSON des arrondissements de Paris\n",
    "arrondissements_geojson = \"arrondissements.geojson\"  \n",
    "arrondissements_gdf = gpd.read_file(arrondissements_geojson)  # Charger les arrondissements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92552/3755955189.py:32: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  if not arrondissements_gdf.geometry.unary_union.contains(center_point):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Fonction pour créer la grille à partir des latitudes et longitudes\n",
    "def creer_grille(lat_min, lat_max, lon_min, lon_max, step):\n",
    "    grille = []\n",
    "    latitudes = np.arange(lat_min, lat_max, step)\n",
    "    longitudes = np.arange(lon_min, lon_max, step)\n",
    "    \n",
    "    for i in range(len(latitudes) - 1):\n",
    "        for j in range(len(longitudes) - 1):\n",
    "            carre = {\n",
    "                \"id\": f\"{i}_{j}\",  # Identifiant unique pour chaque carré\n",
    "                \"lat_min\": latitudes[i],\n",
    "                \"lat_max\": latitudes[i + 1],\n",
    "                \"lon_min\": longitudes[j],\n",
    "                \"lon_max\": longitudes[j + 1],\n",
    "                \"lati\": latitudes[i] + 0.005/2,  # Centre du carré\n",
    "                \"long\": longitudes[j] +  0.005/2,  # Centre du carré\n",
    "            }\n",
    "            grille.append(carre)\n",
    "    return grille\n",
    "\n",
    "# Fonction pour trouver le carré correspondant à un point\n",
    "def trouver_carre(lat, lon, grille):\n",
    "    center_point = Point(lon, lat)\n",
    "    if pd.isnull(lat) or pd.isnull(lon):  # Vérifier si lat ou lon est NaN\n",
    "        return None, None, None\n",
    "    for carre in grille:\n",
    "        if carre[\"lat_min\"] <= lat < carre[\"lat_max\"] and carre[\"lon_min\"] <= lon < carre[\"lon_max\"]:\n",
    "            if not arrondissements_gdf.geometry.unary_union.contains(center_point):\n",
    "                continue  # Ignorer les carrés hors des arrondissements de Paris\n",
    "            return carre[\"id\"], carre[\"lati\"], carre[\"long\"]\n",
    "    return None, None, None\n",
    "\n",
    "# Fonction pour associer le carré à chaque ligne du DataFrame\n",
    "def associer_carres(df, grille):\n",
    "    # Appliquer la fonction trouver_carre à chaque ligne\n",
    "    df[['carre_id', 'lati', 'long']] = df.apply(\n",
    "        lambda row: pd.Series(trouver_carre(row['lat'], row['lon'], grille)), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Définir les limites de la grille\n",
    "lat_min, lat_max = 48.81, 48.92\n",
    "lon_min, lon_max = 2.255, 2.42\n",
    "step = 0.005  # Taille d'un carré\n",
    "\n",
    "# Créer la grille\n",
    "grille = creer_grille(lat_min, lat_max, lon_min, lon_max, step)\n",
    "\n",
    "\n",
    "# Nettoyer les colonnes lat et lon\n",
    "merged_df['lat'] = merged_df['lat'].astype(str).str.strip()\n",
    "merged_df['lon'] = merged_df['lon'].astype(str).str.strip()\n",
    "\n",
    "# Convertir les colonnes lat et lon en float\n",
    "merged_df['lat'] = pd.to_numeric(merged_df['lat'], errors='coerce')\n",
    "merged_df['lon'] = pd.to_numeric(merged_df['lon'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# Associer les carrés\n",
    "merged_df = associer_carres(merged_df, grille)\n",
    "\n",
    "# Supprimer les lignes où carre_id est NaN\n",
    "merged_df = merged_df.dropna(subset=['carre_id'])\n",
    "\n",
    "# Afficher le DataFrame final\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Regrouper les données par carré et t_1h, puis calculer la moyenne de k\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m resultats_groupes \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_1h\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcarre_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m      3\u001b[0m     moyenne_k\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m     somme_k\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     count_k\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m     lati\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlati\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      7\u001b[0m     long\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     10\u001b[0m resultats_groupes\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Regrouper les données par carré et t_1h, puis calculer la moyenne de k\n",
    "resultats_groupes = merged_df.groupby(['t_1h','carre_id']).agg(\n",
    "    moyenne_k=('k', 'mean'),\n",
    "    somme_k=('k', 'sum'),\n",
    "    count_k=('k', 'count'),\n",
    "    lati= ('lati', 'mean'),\n",
    "    long= ('long', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "resultats_groupes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyennes calculées et sauvegardées dans 'moyennes_par_carre_et_t1h.csv'\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "resultats_groupes.to_csv('moyennes_par_carre_et_t1h.csv', index=False)\n",
    "\n",
    "print(\"Moyennes calculées et sauvegardées dans 'moyennes_par_carre_et_t1h.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
