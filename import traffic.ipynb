{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# URL du fichier ZIP\n",
    "url = \"https://opendata.paris.fr/api/datasets/1.0/comptages-routiers-permanents-historique/attachments/opendata_txt_2020_zip/\"\n",
    "\n",
    "# Nom local du fichier ZIP\n",
    "zip_filename = \"comptages_routiers_2020.zip\"\n",
    "\n",
    "# Dossier où extraire les fichiers\n",
    "output_folder = \"extracted_data\"\n",
    "\n",
    "# Nombre de lignes à lire par fichier\n",
    "max_lines = 1000\n",
    "\n",
    "# Étape 1 : Télécharger le fichier ZIP\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Fichier téléchargé : {zip_filename}\")\n",
    "else:\n",
    "    print(f\"Erreur lors du téléchargement : {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Étape 2 : Extraire le contenu du fichier ZIP (limité à deux fichiers)\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    all_files = zip_ref.namelist()[:2]  # Limiter à 2 fichiers\n",
    "    for file in all_files:\n",
    "        zip_ref.extract(file, output_folder)\n",
    "    print(f\"Deux premiers fichiers extraits : {all_files}\")\n",
    "\n",
    "# Étape 3 : Lire partiellement les fichiers texte et les convertir en JSON\n",
    "extracted_files = os.listdir(output_folder)\n",
    "converted_files = []\n",
    "\n",
    "for file in extracted_files:\n",
    "    file_path = os.path.join(output_folder, file)\n",
    "    if file.endswith(\".txt\"):  # Assurez-vous que le fichier est un texte\n",
    "        try:\n",
    "            # Lire les premières lignes seulement\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = [next(f).strip() for _ in range(max_lines) if not f.tell() == os.fstat(f.fileno()).st_size]\n",
    "\n",
    "            # Convertir en JSON\n",
    "            json_data = [line.split(\"\\t\") for line in lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Exemple de grille (adapter aux limites de Paris)\n",
    "grille = [\n",
    "    {\"id\": 1, \"lat_min\": 48.85, \"lat_max\": 48.86, \"lon_min\": 2.29, \"lon_max\": 2.30},\n",
    "    {\"id\": 2, \"lat_min\": 48.86, \"lat_max\": 48.87, \"lon_min\": 2.30, \"lon_max\": 2.31},\n",
    "    # Ajouter autant de carrés nécessaires\n",
    "]\n",
    "\n",
    "# Fonction pour trouver le carré correspondant à un point\n",
    "def trouver_carre(lat, lon, grille):\n",
    "    for carre in grille:\n",
    "        if carre[\"lat_min\"] <= lat < carre[\"lat_max\"] and carre[\"lon_min\"] <= lon < carre[\"lon_max\"]:\n",
    "            return carre[\"id\"]\n",
    "    return None\n",
    "\n",
    "# Charger les données JSON\n",
    "with open(\"data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialiser le regroupement\n",
    "regroupement = {carre[\"id\"]: {\"somme_k\": 0, \"count_k\": 0} for carre in grille}\n",
    "\n",
    "# Parcourir les données pour les regrouper par carré\n",
    "for item in data[\"results\"]:\n",
    "    coord = item[\"geo_point_2d\"]\n",
    "    lat, lon = coord[\"lat\"], coord[\"lon\"]\n",
    "    valeur_k = item.get(\"k\", None)  # Champ \"k\" pour la moyenne\n",
    "    carre_id = trouver_carre(lat, lon, grille)\n",
    "\n",
    "    if carre_id is not None and valeur_k is not None:\n",
    "        regroupement[carre_id][\"somme_k\"] += valeur_k\n",
    "        regroupement[carre_id][\"count_k\"] += 1\n",
    "\n",
    "# Calcul des moyennes pour chaque carré\n",
    "resultats_final = []\n",
    "for carre_id, contenu in regroupement.items():\n",
    "    somme_k = contenu[\"somme_k\"]\n",
    "    count_k = contenu[\"count_k\"]\n",
    "    moyenne_k = somme_k / count_k if count_k > 0 else None\n",
    "    resultats_final.append({\"carre_id\": carre_id, \"moyenne_k\": moyenne_k})\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier JSON\n",
    "with open(\"moyennes_par_carre.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resultats_final, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Moyennes calculées et sauvegardées dans 'moyennes_par_carre.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
